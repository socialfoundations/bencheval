Model	Mean win rate	BoolQ - Stereotypes (race)	BoolQ - Stereotypes (gender)	BoolQ - Representation (race)	BoolQ - Representation (gender)	NarrativeQA - Stereotypes (race)	NarrativeQA - Stereotypes (gender)	NarrativeQA - Representation (race)	NarrativeQA - Representation (gender)	NaturalQuestions (closed-book) - Stereotypes (race)	NaturalQuestions (closed-book) - Stereotypes (gender)	NaturalQuestions (closed-book) - Representation (race)	NaturalQuestions (closed-book) - Representation (gender)	NaturalQuestions (open-book) - Stereotypes (race)	NaturalQuestions (open-book) - Stereotypes (gender)	NaturalQuestions (open-book) - Representation (race)	NaturalQuestions (open-book) - Representation (gender)	QuAC - Stereotypes (race)	QuAC - Stereotypes (gender)	QuAC - Representation (race)	QuAC - Representation (gender)	MS MARCO (regular) - Stereotypes (race)	MS MARCO (regular) - Stereotypes (gender)	MS MARCO (regular) - Representation (race)	MS MARCO (regular) - Representation (gender)	MS MARCO (TREC) - Stereotypes (race)	MS MARCO (TREC) - Stereotypes (gender)	MS MARCO (TREC) - Representation (race)	MS MARCO (TREC) - Representation (gender)	CNN/DailyMail - Stereotypes (race)	CNN/DailyMail - Stereotypes (gender)	CNN/DailyMail - Representation (race)	CNN/DailyMail - Representation (gender)	XSUM - Stereotypes (race)	XSUM - Stereotypes (gender)	XSUM - Representation (race)	XSUM - Representation (gender)	IMDB - Stereotypes (race)	IMDB - Stereotypes (gender)	IMDB - Representation (race)	IMDB - Representation (gender)	CivilComments - Stereotypes (race)	CivilComments - Stereotypes (gender)	CivilComments - Representation (race)	CivilComments - Representation (gender)	RAFT - Stereotypes (race)	RAFT - Stereotypes (gender)	RAFT - Representation (race)	RAFT - Representation (gender)
OPT (66B)	0.631	-	-	-	-	-	0.416	0.556	0.191	-	0.278	0.481	0.156	0.667	0.338	0.427	0.119	0.592	0.413	0.272	0.245	-	-	-	-	-	-	-	-	0.609	0.404	0.337	0.128	0.667	0.469	0.462	0.186	-	-	-	-	-	-	-	-	-	-	-	-
Luminous Extended (30B)	0.629	-	-	-	-	-	0.416	0.556	0.199	0.667	0.46	0.435	0.223	0.667	0.411	0.441	0.045	0.612	0.403	0.24	0.2	-	-	-	-	-	-	-	-	0.608	0.391	0.274	0.151	0.667	0.45	0.547	0.214	-	-	-	-	-	-	-	-	-	-	-	-
T0pp (11B)	0.605	-	-	-	0.25	0.667	0.339	0.667	0.105	0.667	0.462	0.613	0.177	0.329	0.388	0.462	0.091	0.667	0.428	0.436	0.291	-	-	-	-	-	-	-	-	0.594	0.403	0.277	0.093	0.667	0.444	0.457	0.27	-	-	-	-	-	-	-	0.459	-	0.5	-	0.125
Cohere xlarge v20221108 (52.4B)	0.602	-	-	-	-	-	0.472	0.667	0.192	-	0.444	0.48	0.247	0.667	0.232	0.474	0.113	0.571	0.395	0.304	0.233	-	-	-	-	-	-	-	-	0.607	0.383	0.266	0.133	0.667	0.454	0.537	0.218	-	-	-	-	-	-	-	-	-	-	-	-
Cohere xlarge v20220609 (52.4B)	0.598	-	-	-	-	-	0.454	0.556	0.208	-	-	0.43	0.094	0.667	0.388	0.409	0.051	0.582	0.438	0.344	0.23	-	-	-	-	-	-	-	-	0.626	0.387	0.301	0.117	0.667	0.463	0.622	0.205	-	-	-	-	-	-	-	-	-	-	-	-
Jurassic-2 Jumbo (178B)	0.597	-	-	-	-	-	0.43	0.5	0.183	-	0.5	0.376	0.095	0.667	0.413	0.541	0.107	0.642	0.454	0.359	0.232	-	-	-	-	-	-	-	-	0.608	0.411	0.254	0.083	0.667	0.466	0.399	0.205	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (30B)	0.596	-	-	-	-	-	0.4	0.667	0.214	0.667	0.5	0.328	0.333	-	0.1	0.39	0.257	0.571	0.436	0.229	0.222	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Anthropic-LM v4-s3 (52B)	0.593	-	-	-	-	-	0.39	0.667	0.208	-	0.5	0.386	0.148	-	0.429	0.48	0.043	0.609	0.419	0.321	0.248	-	-	-	-	-	-	-	-	0.616	0.412	0.252	0.093	0.667	0.439	0.541	0.207	-	-	-	-	-	-	-	-	-	-	-	-
J1-Grande v2 beta (17B)	0.592	-	-	-	-	-	0.3	0.667	0.179	0.667	0.5	0.392	0.174	0.667	0.167	0.488	0.381	0.628	0.411	0.327	0.225	-	-	-	-	-	-	-	-	0.615	0.401	0.293	0.099	0.667	0.465	0.522	0.214	-	-	-	-	-	-	-	-	-	-	-	-
OPT (175B)	0.58	-	-	-	-	-	0.491	0.667	0.232	-	0.327	0.521	0.081	-	0.439	0.461	0.325	0.591	0.386	0.243	0.207	-	-	-	-	-	-	-	-	0.591	0.407	0.294	0.123	0.667	0.449	0.453	0.218	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (13B)	0.578	-	-	-	-	-	0.417	-	0.224	0.667	0.5	0.438	0.167	-	-	0.333	0.083	0.605	0.444	0.276	0.224	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Cohere Command beta (52.4B)	0.576	-	-	-	-	-	0.404	0.667	0.178	0.667	0.5	0.552	0.129	0.667	0.482	0.579	0.05	0.596	0.47	0.316	0.232	-	-	-	-	-	-	-	-	0.612	0.396	0.286	0.09	0.667	0.457	0.522	0.181	-	-	-	-	-	-	-	-	-	-	-	-
Luminous Supreme (70B)	0.571	-	-	-	-	-	0.465	0.667	0.238	0.667	0.446	0.48	0.125	0.667	0.444	0.44	0.22	0.598	0.412	0.305	0.232	-	-	-	-	-	-	-	-	0.63	0.401	0.291	0.13	0.667	0.439	0.544	0.206	-	-	-	-	-	-	-	-	-	-	-	-
Jurassic-2 Grande (17B)	0.562	-	-	-	-	-	0.448	0.667	0.196	0.667	0.5	0.507	0.176	0.667	0.5	0.465	0.03	0.64	0.422	0.23	0.224	-	-	-	-	-	-	-	-	0.636	0.402	0.359	0.117	0.667	0.456	0.466	0.207	-	-	-	-	-	-	-	-	-	-	-	-
UL2 (20B)	0.56	-	-	-	0.23	0.667	0.337	0.342	0.154	0.667	0.387	0.519	0.183	0.667	0.449	0.538	0.111	0.614	0.402	0.317	0.253	-	-	-	-	-	-	-	-	0.667	0.402	0.361	0.188	0.667	0.455	0.524	0.251	-	-	-	-	-	-	-	-	-	-	-	0.079
J1-Grande v1 (17B)	0.558	-	-	-	-	-	0.5	0.667	0.164	-	0.5	0.521	0.033	0.667	0.346	0.488	0.113	0.6	0.428	0.34	0.242	-	-	-	-	-	-	-	-	0.633	0.4	0.351	0.13	0.667	0.442	0.557	0.171	-	-	-	-	-	-	-	-	-	-	-	-
Cohere medium v20221108 (6.1B)	0.556	-	-	-	-	-	0.441	0.667	0.181	-	0.45	0.451	0.314	-	0.308	0.452	0.061	0.651	0.441	0.353	0.251	-	-	-	-	-	-	-	-	0.612	0.408	0.287	0.141	0.667	0.436	0.393	0.194	-	-	-	-	-	-	-	-	-	-	-	-
Luminous Base (13B)	0.552	-	-	-	-	-	0.438	0.556	0.172	-	0.417	0.433	0.162	0.667	0.432	0.457	0.32	0.658	0.417	0.32	0.203	-	-	-	-	-	-	-	-	0.629	0.408	0.287	0.164	0.667	0.442	0.667	0.165	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (7B)	0.55	-	-	-	-	-	0.444	-	0.178	0.667	0.5	0.374	0.111	-	0.3	0.506	0.36	0.571	0.428	0.284	0.259	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
J1-Large v1 (7.5B)	0.549	-	-	-	-	-	0.5	0.667	0.203	-	0.405	0.362	0.216	0.667	0.5	0.394	0.109	0.647	0.428	0.3	0.249	-	-	-	-	-	-	-	-	0.632	0.391	0.302	0.142	0.667	0.424	0.426	0.172	-	-	-	-	-	-	-	-	-	-	-	-
J1-Jumbo v1 (178B)	0.549	-	-	-	-	-	0.438	0.667	0.214	-	0.5	0.333	0.175	0.667	0.46	0.478	0.041	0.604	0.42	0.329	0.242	-	-	-	-	-	-	-	-	0.63	0.386	0.325	0.131	0.667	0.472	0.48	0.186	-	-	-	-	-	-	-	-	-	-	-	-
BLOOM (176B)	0.546	-	-	-	-	-	0.355	0.667	0.165	-	0.5	0.418	0.09	0.667	0.426	0.499	0.135	0.631	0.396	0.365	0.244	-	-	-	-	-	-	-	-	0.658	0.385	0.314	0.145	-	0.467	0.309	0.172	-	-	-	-	-	-	-	-	-	-	-	-
Palmyra X (43B)	0.546	-	-	-	-	0.667	0.398	0.667	0.159	0.667	0.5	0.314	0.266	-	-	-	-	0.642	0.395	0.293	0.235	-	-	-	-	-	-	-	-	0.622	0.421	0.276	0.114	0.667	0.438	0.439	0.205	-	-	-	-	-	-	-	-	-	-	-	-
Cohere large v20220720 (13.1B)	0.541	-	-	-	-	-	0.473	0.667	0.202	-	0.333	0.34	0.233	0.667	0.39	0.457	0.174	0.667	0.441	0.338	0.238	-	-	-	-	-	-	-	-	0.626	0.401	0.238	0.134	0.667	0.466	0.667	0.157	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (70B)	0.538	-	-	-	-	-	0.5	-	0.187	0.667	0.167	0.524	0.313	-	-	0.566	0.184	0.611	0.403	0.272	0.239	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Vicuna v1.3 (13B)	0.533	-	-	-	-	-	0.417	0.667	0.181	0.667	-	0.364	0.132	0.667	0.5	0.484	0.293	0.63	0.408	0.289	0.242	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
text-babbage-001	0.533	-	-	-	-	0.667	0.403	0.667	0.132	-	0.5	0.317	0.145	0.667	0.333	0.403	0.243	0.617	0.435	0.361	0.26	-	-	-	-	-	-	-	-	0.626	0.385	0.389	0.147	0.667	0.443	0.521	0.204	-	-	-	-	-	-	-	-	-	-	-	-
TNLG v2 (530B)	0.531	-	-	-	-	-	0.395	-	0.221	-	0.342	0.559	0.289	0.667	0.277	0.469	0.259	0.579	0.435	0.333	0.25	-	-	-	-	-	-	-	-	0.629	0.398	0.227	0.12	0.667	0.449	0.486	0.204	-	-	-	-	-	-	-	-	-	-	-	-
Mistral v0.1 (7B)	0.525	-	-	-	-	-	0.5	0.667	0.173	0.667	0.25	0.287	0.065	0.667	0.5	0.439	0.48	0.621	0.412	0.274	0.248	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
gpt-3.5-turbo-0613	0.523	-	-	-	-	0.667	0.455	0.429	0.169	0.667	0.5	0.382	0.104	0.667	0.5	0.415	0.233	0.589	0.403	0.378	0.223	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon-Instruct (7B)	0.515	-	-	-	-	-	0.444	-	0.187	0.667	0.5	0.272	0.071	-	0.5	0.426	0.068	0.625	0.456	0.262	0.251	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
gpt-3.5-turbo-0301	0.513	-	-	-	0.5	-	0.479	0.333	0.216	-	0.353	0.364	0.167	0.667	0.5	0.408	0.236	0.639	0.403	0.436	0.229	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Pythia (6.9B)	0.51	-	-	-	-	-	0.444	0.667	0.204	-	0.5	0.312	0.188	-	0.387	0.422	0.159	0.635	0.416	0.369	0.25	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	0.5
Vicuna v1.3 (7B)	0.509	-	-	-	-	-	0.379	0.373	0.186	0.667	0.333	0.55	0.324	0.667	0.5	0.521	0.458	0.633	0.416	0.277	0.255	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
text-davinci-002	0.502	-	-	-	-	-	0.395	0.667	0.189	0.667	-	0.448	0.129	0.667	0.407	0.487	0.401	0.579	0.453	0.27	0.255	-	-	-	-	-	-	-	-	0.625	0.408	0.293	0.107	0.667	0.457	0.481	0.239	-	-	-	-	-	-	-	-	-	-	-	-
text-curie-001	0.495	-	-	-	-	0.667	0.446	0.609	0.19	-	0.5	0.566	0.238	-	0.433	0.441	0.158	0.631	0.456	0.274	0.242	-	-	-	-	-	-	-	-	0.61	0.387	0.301	0.118	0.667	0.442	0.54	0.194	-	-	-	-	-	-	-	-	-	-	-	-
T5 (11B)	0.489	-	-	0.667	0.375	0.667	0.408	0.367	0.156	-	0.5	0.533	0.103	0.667	0.417	0.516	0.243	0.65	0.44	0.397	0.257	-	-	-	-	-	-	-	-	0.632	0.452	0.264	0.119	0.667	0.5	0.358	0.222	-	-	-	-	-	-	-	-	-	-	-	0.5
Alpaca (7B)	0.487	-	-	-	0.5	0.667	0.41	0.667	0.196	0.667	0.456	0.412	0.054	0.667	0.419	0.454	0.315	0.636	0.435	0.236	0.281	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	0.5
Falcon (40B)	0.486	-	-	-	-	0.667	0.398	0.667	0.191	0.667	0.5	0.256	0.107	0.667	0.443	0.382	0.132	-	0.468	0.423	0.141	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
babbage (1.3B)	0.48	-	-	-	-	0.667	0.445	0.667	0.191	-	0.5	0.624	0.015	0.667	0.479	0.441	0.349	0.659	0.445	0.339	0.258	-	-	-	-	-	-	-	-	0.568	0.418	0.327	0.146	0.667	0.42	0.458	0.148	-	-	-	-	-	-	-	-	-	-	-	0
InstructPalmyra (30B)	0.471	-	-	-	-	-	0.445	0.444	0.196	0.667	0.5	0.525	0.134	-	0.392	0.49	0.384	0.582	0.431	0.337	0.236	-	-	-	-	-	-	-	-	0.638	0.371	0.258	0.117	0.667	0.459	0.59	0.187	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (13B)	0.469	-	-	-	-	-	0.417	-	0.218	0.667	-	0.521	0.15	0.667	0.5	0.467	0.357	0.549	0.392	0.325	0.242	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
GPT-NeoX (20B)	0.468	-	-	-	-	0.667	0.449	0.667	0.186	-	0.5	0.362	0.318	0.667	0.5	0.57	0.094	0.626	0.448	0.334	0.268	-	-	-	-	-	-	-	-	0.616	0.41	0.289	0.149	0.667	0.449	0.526	0.162	-	-	-	-	-	-	-	-	-	-	-	-
Cohere medium v20220720 (6.1B)	0.466	-	-	-	-	0.667	0.427	0.569	0.174	-	0.5	0.441	0.251	-	0.354	0.325	0.234	0.667	0.412	0.357	0.262	-	-	-	-	-	-	-	-	0.659	0.44	0.304	0.173	0.667	0.461	0.498	0.186	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Base-v1 (3B)	0.46	-	-	-	-	-	0.463	0.667	0.167	-	0.5	0.46	0.182	0.667	0.464	0.453	0.242	0.575	0.389	0.3	0.269	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (7B)	0.458	-	-	-	-	-	0.333	0.667	0.203	-	0.5	0.381	0.182	0.667	0.5	0.577	0.486	0.583	0.426	0.283	0.231	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
GPT-J (6B)	0.457	-	-	-	-	0.667	0.451	0.667	0.217	0.667	0.5	0.49	0.192	0.667	0.5	0.524	0.317	0.613	0.43	0.266	0.23	-	-	-	-	-	-	-	-	0.63	0.402	0.293	0.146	0.667	0.435	0.513	0.165	-	-	-	-	-	-	0.5	0.5	-	-	-	-
Cohere small v20220720 (410M)	0.452	-	-	-	-	0.667	0.418	0.556	0.202	-	0.5	0.415	0.234	0.667	0.485	0.435	0.265	0.667	0.458	0.341	0.285	-	-	-	-	-	-	-	-	0.648	0.42	0.145	0.182	0.667	0.43	0.556	0.246	-	-	-	-	-	-	-	-	-	-	-	0.5
GLM (130B)	0.451	-	-	-	-	-	0.372	0.667	0.19	-	0.5	0.269	0.059	0.667	0.5	0.585	0.073	0.62	0.431	0.408	0.268	-	-	-	-	-	-	-	-	0.611	0.394	0.29	0.139	0.667	0.447	0.545	0.207	-	-	-	-	-	-	-	-	-	-	-	-
Jurassic-2 Large (7.5B)	0.446	-	-	-	-	-	-	-	-	-	0.5	0.531	0.079	0.667	0.433	0.504	0.203	-	-	-	-	-	-	-	-	-	-	-	-	0.647	0.405	0.245	0.133	0.667	0.464	0.58	0.22	-	-	-	-	-	-	-	-	-	-	-	-
davinci (175B)	0.445	-	-	-	-	-	0.443	0.667	0.208	-	0.447	0.382	0.247	0.667	0.365	0.435	0.244	0.65	0.445	0.367	0.251	-	-	-	-	-	-	-	-	0.619	0.401	0.301	0.125	0.667	0.444	0.564	0.217	-	-	-	-	-	-	-	-	-	-	-	-
TNLG v2 (6.7B)	0.437	-	-	-	-	-	0.476	0.667	0.212	-	0.498	0.479	0.274	0.667	0.333	0.446	0.228	0.618	0.472	0.351	0.232	-	-	-	-	-	-	-	-	0.616	0.404	0.326	0.146	0.667	0.462	0.489	0.182	-	-	-	-	-	-	-	-	-	-	-	-
Cohere Command beta (6.1B)	0.436	-	-	-	-	-	0.488	0.667	0.144	-	-	0.465	0.183	0.667	0.5	0.487	0.356	0.667	0.471	0.356	0.248	-	-	-	-	-	-	-	-	0.603	0.408	0.259	0.121	0.667	0.454	0.505	0.215	-	-	-	-	-	-	-	-	-	-	-	-
Pythia (12B)	0.433	-	-	-	-	-	0.5	0.667	0.215	-	0.5	0.407	0.122	-	0.405	0.467	0.276	0.641	0.415	0.314	0.26	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon-Instruct (40B)	0.431	-	-	-	-	0.667	0.332	0.467	0.175	0.667	0.5	0.415	0.155	0.667	0.42	0.552	0.195	0.667	0.418	0.476	0.214	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
ada (350M)	0.427	-	-	-	-	0.667	0.444	0.667	0.132	-	0.5	0.284	0.281	0.667	0.496	0.466	0.333	0.667	0.452	0.341	0.209	-	-	-	-	-	-	-	-	0.628	0.403	0.297	0.134	0.667	0.412	0.558	0.222	-	-	-	-	-	-	-	-	-	-	-	0.5
text-ada-001	0.426	-	-	-	-	-	0.403	0.667	0.203	-	-	0.667	0.167	-	0.5	0.633	0.217	0.653	0.433	0.345	0.244	-	-	-	-	-	-	-	-	0.603	0.376	0.327	0.135	0.667	0.403	0.597	0.19	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Base (7B)	0.414	-	-	-	-	-	0.438	0.667	0.171	-	-	0.498	0.289	0.667	0.408	0.412	0.256	0.667	0.382	0.38	0.249	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
text-davinci-003	0.409	-	-	-	0.5	0.667	0.442	0.667	0.177	0.667	0.484	0.347	0.27	0.667	0.5	0.443	0.407	0.582	0.428	0.369	0.257	-	-	-	-	-	-	-	-	0.646	0.414	0.274	0.083	0.667	0.449	0.534	0.238	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (65B)	0.406	-	-	-	-	-	0.5	0.667	0.198	-	-	0.352	0.3	-	0.5	0.436	0.393	0.621	0.394	0.38	0.243	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
YaLM (100B)	0.379	-	-	-	-	0.667	0.449	0.568	0.177	0.667	0.478	0.327	0.168	0.667	0.5	0.385	0.175	0.667	0.454	0.465	0.343	-	-	-	-	-	-	-	-	0.667	0.42	0.588	0.206	0.667	0.442	0.501	0.248	-	-	-	-	-	-	-	-	-	-	-	0.5
curie (6.7B)	0.366	-	-	-	-	-	0.455	0.667	0.229	-	0.5	0.415	0.203	0.667	0.469	0.453	0.379	0.645	0.439	0.246	0.231	-	-	-	-	-	-	-	-	0.642	0.409	0.295	0.129	0.667	0.449	0.599	0.205	-	-	-	-	-	-	-	-	-	-	-	-
MPT-Instruct (30B)	0.362	-	-	-	-	-	0.5	-	0.224	-	0	0.542	0.091	0.667	0.5	0.493	0.286	0.667	0.426	0.407	0.232	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon (7B)	0.356	-	-	-	-	-	0.444	0.667	0.205	-	0.389	0.476	0.14	-	0.333	0.553	0.275	0.667	0.457	0.402	0.247	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Instruct (7B)	0.338	-	-	-	-	-	0.5	-	0.193	0.667	0.5	0.406	0	0.667	0.5	0.524	0.281	0.63	0.445	0.333	0.242	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
MPT (30B)	0.295	-	-	-	-	-	0.5	-	0.238	-	-	0.333	0.088	0.667	0.5	0.527	0.18	0.667	0.413	0.443	0.279	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Instruct-v1 (3B)	0.19	-	-	-	-	-	0.5	0.667	0.184	-	0.5	0.467	0.278	0.667	0.5	0.566	0.324	0.667	0.439	0.34	0.285	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-

