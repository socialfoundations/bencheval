Model	Mean win rate	MMLU - ECE (10-bin)	BoolQ - ECE (10-bin)	NarrativeQA - ECE (10-bin)	NaturalQuestions (closed-book) - ECE (10-bin)	NaturalQuestions (open-book) - ECE (10-bin)	QuAC - ECE (10-bin)	HellaSwag - ECE (10-bin)	OpenbookQA - ECE (10-bin)	TruthfulQA - ECE (10-bin)	IMDB - ECE (10-bin)	CivilComments - ECE (10-bin)	RAFT - ECE (10-bin)
T0pp (11B)	0.758	0.168	0.322	0	0	0	0.001	-	-	0.154	0.291	0.308	0.086
J1-Jumbo v1 (178B)	0.666	0.131	0.215	0.034	0.035	0.065	0.043	0.217	0.25	0.113	0.064	0.27	0.228
Jurassic-2 Jumbo (178B)	0.66	0.137	0.175	0.073	0.018	0.073	0.035	-	-	0.068	0.182	0.314	0.218
Cohere large v20220720 (13.1B)	0.652	0.112	0.088	0.037	0.025	0.143	0.033	0.288	0.225	0.105	0.132	0.384	0.267
GLM (130B)	0.652	0.128	0.171	0.037	0.022	0.076	0.027	-	-	0.088	0.18	0.486	0.226
Jurassic-2 Large (7.5B)	0.644	0.141	0.147	-	0.014	0.084	-	-	-	0.102	0.178	0.19	0.254
Luminous Base (13B)	0.641	0.111	0.066	0.048	0.045	0.07	0.098	-	-	0.081	0.232	0.28	0.29
J1-Large v1 (7.5B)	0.638	0.123	0.106	0.046	0.015	0.086	0.024	0.192	0.25	0.112	0.213	0.377	0.269
J1-Grande v2 beta (17B)	0.634	0.139	0.167	0.041	0.036	0.065	0.04	0.226	0.215	0.123	0.136	0.376	0.234
Jurassic-2 Grande (17B)	0.63	0.134	0.209	0.126	0.018	0.063	0.035	-	-	0.097	0.111	0.381	0.232
Luminous Supreme (70B)	0.624	0.154	0.083	0.049	0.041	0.074	0.058	-	-	0.092	0.173	0.272	0.238
J1-Grande v1 (17B)	0.622	0.114	0.154	0.047	0.029	0.081	0.036	0.213	0.258	0.091	0.158	0.408	0.244
ada (350M)	0.616	0.128	0.067	0.046	0.028	0.18	0.039	0.057	0.346	0.071	0.274	0.355	0.268
TNLG v2 (530B)	0.615	0.127	0.048	0.05	0.04	0.075	0.08	0.322	0.243	0.226	0.087	0.213	0.244
Cohere small v20220720 (410M)	0.609	0.136	0.095	0.031	0.023	0.198	0.036	0.083	0.379	0.076	0.134	0.486	0.234
curie (6.7B)	0.603	0.138	0.079	0.045	0.017	0.134	0.043	0.25	0.26	0.062	0.259	0.293	0.319
TNLG v2 (6.7B)	0.602	0.132	0.065	0.046	0.031	0.089	0.056	0.268	0.282	0.117	0.118	0.248	0.314
Cohere medium v20221108 (6.1B)	0.601	0.113	0.095	0.028	0.015	0.233	0.041	0.281	0.23	0.08	0.36	0.487	0.253
Cohere Command beta (52.4B)	0.596	0.183	0.023	0.058	0.084	0.056	0.06	0.325	0.231	0.311	0.015	0.161	0.262
babbage (1.3B)	0.588	0.14	0.068	0.027	0.016	0.147	0.045	0.144	0.3	0.142	0.212	0.31	0.286
Cohere xlarge v20221108 (52.4B)	0.585	0.143	0.051	0.059	0.054	0.073	0.063	0.333	0.207	0.211	0.069	0.313	0.25
Luminous Extended (30B)	0.577	0.135	0.129	0.046	0.022	0.09	0.096	-	-	0.064	0.204	0.359	0.29
davinci (175B)	0.575	0.132	0.072	0.067	0.061	0.079	0.068	0.31	0.204	0.211	0.126	0.396	0.222
Cohere xlarge v20220609 (52.4B)	0.543	0.149	0.04	0.062	0.068	0.085	0.067	0.341	0.235	0.099	0.069	0.327	0.274
Cohere Command beta (6.1B)	0.529	0.155	0.059	0.076	0.042	0.057	0.062	0.293	0.25	0.3	0.014	0.358	0.274
Cohere medium v20220720 (6.1B)	0.51	0.114	0.082	0.047	0.026	0.142	0.048	0.271	0.275	0.094	0.36	0.459	0.304
text-davinci-002	0.474	0.176	0.064	0.239	0.341	0.242	0.274	0.286	0.238	0.199	0.031	0.183	0.212
UL2 (20B)	0.464	0.134	0.46	0	0.092	0.179	0	-	-	0.125	0.225	0.404	0.401
GPT-J (6B)	0.464	0.115	0.062	0.199	0.075	0.354	0.13	0.233	0.235	0.078	0.295	0.409	0.389
RedPajama-INCITE-Base-v1 (3B)	0.439	0.115	0.187	0.234	0.116	0.345	0.078	-	-	0.048	0.248	0.303	0.502
T5 (11B)	0.435	0.151	0.433	0	0.076	0.239	0	-	-	0.143	0.236	0.38	0.367
Pythia (6.9B)	0.43	0.136	0.106	0.217	0.07	0.369	0.1	-	-	0.076	0.302	0.259	0.502
GPT-NeoX (20B)	0.422	0.122	0.195	0.224	0.103	0.373	0.115	0.277	0.232	0.058	0.23	0.444	0.324
RedPajama-INCITE-Base (7B)	0.409	0.098	0.127	0.276	0.127	0.396	0.131	-	-	0.063	0.206	0.305	0.648
text-davinci-003	0.407	0.317	0.098	0.37	0.286	0.323	0.27	0.278	0.216	0.348	0.113	0.292	0.203
YaLM (100B)	0.402	0.708	0.147	0.06	0.02	0.086	0.029	-	-	0.679	0.418	0.437	0.278
RedPajama-INCITE-Instruct (7B)	0.388	0.143	0.035	0.247	0.142	0.466	0.074	-	-	0.232	0.159	0.102	0.695
Pythia (12B)	0.374	0.111	0.14	0.239	0.094	0.39	0.138	-	-	0.094	0.342	0.297	0.514
RedPajama-INCITE-Instruct-v1 (3B)	0.372	0.124	0.141	0.254	0.12	0.454	0.1	-	-	0.097	0.04	0.383	0.661
BLOOM (176B)	0.348	0.137	0.209	0.237	0.116	0.347	0.122	0.293	0.248	0.096	0.343	0.262	0.44
OPT (175B)	0.338	0.147	0.194	0.254	0.173	0.372	0.148	0.325	0.209	0.054	0.19	0.462	0.352
text-curie-001	0.335	0.462	0.253	0.221	0.253	0.216	0.254	0.153	0.321	0.355	0.031	0.262	0.409
Alpaca (7B)	0.334	0.234	0.343	0.046	0.134	0.238	0.04	-	-	0.375	0.281	0.352	0.33
OPT (66B)	0.289	0.135	0.2	0.245	0.141	0.384	0.154	0.293	0.237	0.073	0.302	0.474	0.468
text-babbage-001	0.277	0.311	0.344	0.186	0.522	0.385	0.24	0.083	0.362	0.251	0.038	0.499	0.295
Vicuna v1.3 (13B)	0.275	0.194	0.159	0.257	0.202	0.43	0.103	-	-	0.316	0.183	0.253	0.376
Vicuna v1.3 (7B)	0.204	0.176	0.322	0.084	0.162	0.413	0.109	-	-	0.227	0.348	0.346	0.601
text-ada-001	0.171	0.506	0.346	0.319	0.764	0.691	0.268	0.103	0.487	0.465	0.09	0.479	0.473
Anthropic-LM v4-s3 (52B)	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (13B)	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (65B)	-	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (13B)	-	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (70B)	-	-	-	-	-	-	-	-	-	-	-	-	-
Mistral v0.1 (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-
gpt-3.5-turbo-0301	-	-	-	-	-	-	-	-	-	-	-	-	-
gpt-3.5-turbo-0613	-	-	-	-	-	-	-	-	-	-	-	-	-
MPT (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-
MPT-Instruct (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon-Instruct (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon (40B)	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon-Instruct (40B)	-	-	-	-	-	-	-	-	-	-	-	-	-
InstructPalmyra (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-
Palmyra X (43B)	-	-	-	-	-	-	-	-	-	-	-	-	-

