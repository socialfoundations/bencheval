Model	Mean win rate	MMLU - EM (Robustness)	BoolQ - EM (Robustness)	NarrativeQA - F1 (Robustness)	NaturalQuestions (closed-book) - F1 (Robustness)	NaturalQuestions (open-book) - F1 (Robustness)	QuAC - F1 (Robustness)	HellaSwag - EM (Robustness)	OpenbookQA - EM (Robustness)	TruthfulQA - EM (Robustness)	MS MARCO (regular) - RR@10 (Robustness)	MS MARCO (TREC) - NDCG@10 (Robustness)	IMDB - EM (Robustness)	CivilComments - EM (Robustness)	RAFT - EM (Robustness)
Llama 2 (70B)	0.965	0.545	0.863	0.722	0.42	0.639	0.362	-	-	0.468	-	-	0.949	0.59	0.673
text-davinci-002	0.916	0.525	0.841	0.638	0.299	0.665	0.319	0.776	0.52	0.547	0.344	0.628	0.925	0.567	0.666
text-davinci-003	0.91	0.517	0.858	0.694	0.369	0.73	0.42	0.798	0.572	0.516	0.304	0.616	0.779	0.594	0.714
Mistral v0.1 (7B)	0.896	0.533	0.837	0.649	0.305	0.631	0.31	-	-	0.339	-	-	0.954	0.521	0.652
LLaMA (65B)	0.885	0.504	0.84	0.567	0.388	0.624	0.275	-	-	0.448	-	-	0.935	0.566	0.655
Cohere Command beta (52.4B)	0.85	0.387	0.811	0.57	0.289	0.679	0.238	0.774	0.492	0.229	0.434	0.734	0.933	0.535	0.599
Llama 2 (13B)	0.823	0.444	0.753	0.682	0.324	0.563	0.294	-	-	0.287	-	-	0.954	0.47	0.652
Palmyra X (43B)	0.821	0.566	0.878	0.672	0.363	-	0.383	-	-	0.568	-	-	0.904	0.006	0.677
Anthropic-LM v4-s3 (52B)	0.818	0.434	0.756	0.663	0.245	0.632	0.313	0.766	0.472	0.326	-	-	0.928	0.514	0.6
gpt-3.5-turbo-0301	0.816	0.525	0.66	0.602	0.327	0.556	0.411	-	-	0.566	-	-	0.857	0.605	0.705
LLaMA (30B)	0.815	0.461	0.791	0.611	0.36	0.612	0.273	-	-	0.281	-	-	0.893	0.503	0.67
Jurassic-2 Jumbo (178B)	0.791	0.417	0.729	0.66	0.315	0.599	0.314	0.754	0.47	0.39	0.337	0.607	0.896	0.449	0.69
Jurassic-2 Grande (17B)	0.764	0.411	0.729	0.583	0.285	0.564	0.276	0.755	0.474	0.293	0.227	0.423	0.928	0.488	0.618
Falcon-Instruct (40B)	0.763	0.446	0.781	0.508	0.335	0.591	0.212	-	-	0.338	-	-	0.938	0.523	0.523
gpt-3.5-turbo-0613	0.762	0.262	0.845	0.566	0.284	0.606	0.371	-	-	0.187	-	-	0.916	0.564	0.677
Vicuna v1.3 (13B)	0.732	0.413	0.757	0.525	0.273	0.621	0.247	-	-	0.341	-	-	0.674	0.593	0.591
J1-Grande v2 beta (17B)	0.711	0.392	0.692	0.565	0.235	0.56	0.251	0.732	0.474	0.252	0.222	0.407	0.947	0.495	0.555
Falcon (40B)	0.705	0.457	0.763	0.557	0.329	0.593	0.162	-	-	0.303	-	-	0.935	0.412	0.586
MPT (30B)	0.697	0.381	0.656	0.584	0.272	0.609	0.231	-	-	0.177	-	-	0.942	0.484	0.58
Vicuna v1.3 (7B)	0.662	0.371	0.672	0.5	0.214	0.539	0.25	-	-	0.258	-	-	0.882	0.543	0.6
MPT-Instruct (30B)	0.656	0.383	0.77	0.623	0.202	0.607	0.204	-	-	0.177	-	-	0.942	0.408	0.548
TNLG v2 (530B)	0.65	0.403	0.733	0.319	0.307	0.525	0.194	0.757	0.476	0.202	0.287	0.565	0.921	0.409	0.545
GLM (130B)	0.647	0.32	0.728	0.629	0.117	0.6	0.193	-	-	0.196	-	-	0.938	0.5	0.577
Llama 2 (7B)	0.644	0.373	0.676	0.573	0.261	0.501	0.271	-	-	0.234	-	-	0.808	0.516	0.573
LLaMA (13B)	0.637	0.37	0.67	0.544	0.272	0.556	0.194	-	-	0.274	-	-	0.875	0.529	0.559
Cohere Command beta (6.1B)	0.616	0.334	0.725	0.529	0.163	0.605	0.17	0.696	0.448	0.171	0.387	0.685	0.921	0.468	0.552
Cohere xlarge v20221108 (52.4B)	0.596	0.299	0.718	0.39	0.283	0.533	0.229	0.764	0.482	0.116	0.242	0.482	0.923	0.408	0.489
LLaMA (7B)	0.568	0.268	0.688	0.485	0.222	0.519	0.223	-	-	0.229	-	-	0.897	0.492	0.486
Luminous Supreme (70B)	0.546	0.255	0.665	0.59	0.252	0.586	0.233	-	-	0.106	-	-	0.932	0.263	0.564
BLOOM (176B)	0.541	0.25	0.642	0.53	0.185	0.558	0.234	0.699	0.438	0.183	0.19	0.333	0.92	0.467	0.527
Jurassic-2 Large (7.5B)	0.527	0.263	0.607	-	0.187	0.503	-	0.687	0.448	0.21	0.177	0.397	0.941	0.469	0.498
InstructPalmyra (30B)	0.522	0.348	0.656	0.317	0.267	0.567	0.248	-	-	0.151	-	-	0.906	0.443	0.518
OPT (175B)	0.519	0.27	0.623	0.409	0.208	0.408	0.2	0.744	0.488	0.205	0.235	0.408	0.919	0.184	0.48
davinci (175B)	0.509	0.34	0.639	0.498	0.256	0.521	0.208	0.738	0.474	0.145	0.154	0.332	0.873	0.461	0.505
Cohere xlarge v20220609 (52.4B)	0.506	0.29	0.614	0.383	0.238	0.471	0.215	0.759	0.448	0.151	0.207	0.397	0.923	0.32	0.563
RedPajama-INCITE-Instruct (7B)	0.495	0.291	0.599	0.482	0.137	0.547	0.164	-	-	0.197	-	-	0.82	0.527	0.605
J1-Jumbo v1 (178B)	0.452	0.221	0.65	0.523	0.179	0.503	0.222	0.726	0.43	0.154	0.144	0.307	0.923	0.271	0.555
OPT (66B)	0.438	0.216	0.683	0.397	0.206	0.458	0.199	0.699	0.45	0.174	0.179	0.437	0.886	0.305	0.405
Luminous Extended (30B)	0.43	0.23	0.659	0.513	0.212	0.524	0.193	-	-	0.151	-	-	0.92	0.368	0.436
Falcon (7B)	0.425	0.236	0.65	0.436	0.185	0.489	0.164	-	-	0.205	-	-	0.692	0.485	0.516
J1-Grande v1 (17B)	0.423	0.225	0.643	0.477	0.17	0.478	0.219	0.695	0.424	0.142	0.121	0.297	0.941	0.417	0.513
RedPajama-INCITE-Instruct-v1 (3B)	0.387	0.218	0.629	0.403	0.132	0.536	0.137	-	-	0.173	-	-	0.852	0.506	0.548
Alpaca (7B)	0.379	0.324	0.643	0.246	0.203	0.491	0.16	-	-	0.199	-	-	0.561	0.482	0.42
Cohere large v20220720 (13.1B)	0.345	0.253	0.545	0.357	0.172	0.347	0.204	0.687	0.43	0.154	0.13	0.257	0.902	0.333	0.49
text-curie-001	0.337	0.22	0.549	0.34	0.121	0.415	0.169	0.625	0.424	0.235	0.198	0.444	0.881	0.129	0.399
GPT-NeoX (20B)	0.336	0.189	0.551	0.421	0.133	0.452	0.191	0.661	0.414	0.175	0.096	0.351	0.912	0.48	0.399
RedPajama-INCITE-Base (7B)	0.331	0.25	0.569	0.424	0.167	0.472	0.186	-	-	0.173	-	-	0.56	0.401	0.489
Luminous Base (13B)	0.319	0.183	0.655	0.476	0.163	0.491	0.185	-	-	0.112	-	-	0.887	0.416	0.402
Falcon-Instruct (7B)	0.303	0.25	0.593	0.258	0.132	0.327	0.179	-	-	0.17	-	-	0.759	0.487	0.445
J1-Large v1 (7.5B)	0.298	0.2	0.567	0.4	0.098	0.41	0.197	0.646	0.412	0.155	0.105	0.248	0.932	0.444	0.443
RedPajama-INCITE-Base-v1 (3B)	0.293	0.217	0.585	0.346	0.134	0.396	0.177	-	-	0.226	-	-	0.843	0.336	0.427
GPT-J (6B)	0.291	0.217	0.621	0.135	0.099	0.228	0.147	0.619	0.398	0.181	0.116	0.319	0.903	0.418	0.53
Pythia (12B)	0.272	0.22	0.51	0.42	0.108	0.47	0.171	-	-	0.138	-	-	0.854	0.418	0.45
Cohere medium v20221108 (6.1B)	0.27	0.207	0.54	0.296	0.105	0.222	0.152	0.687	0.414	0.17	0.13	0.314	0.888	0.353	0.502
UL2 (20B)	0.257	0.272	0.646	0.059	0.141	0.291	0.111	-	-	0.178	-	-	0.276	0.45	0.349
TNLG v2 (6.7B)	0.24	0.169	0.638	0.352	0.149	0.299	0.159	0.656	0.408	0.136	0.105	0.278	0.896	0.336	0.445
curie (6.7B)	0.231	0.19	0.545	0.367	0.126	0.338	0.171	0.632	0.396	0.186	0.11	0.253	0.803	0.347	0.413
T0pp (11B)	0.228	0.378	0	0.099	0.031	0.122	0.071	-	-	0.365	-	-	0.17	0.087	0.085
text-babbage-001	0.226	0.186	0.384	0.126	0.04	0.151	0.087	0.468	0.39	0.195	0.122	0.356	0.844	0.499	0.383
YaLM (100B)	0.205	0.243	0.566	0.088	0.047	0.125	0.08	-	-	0.202	-	-	0.719	0.463	0.211
Cohere medium v20220720 (6.1B)	0.188	0.184	0.562	0.3	0.102	0.266	0.144	0.651	0.382	0.149	0.109	0.315	0.889	0.136	0.385
Pythia (6.9B)	0.182	0.201	0.527	0.313	0.094	0.391	0.171	-	-	0.139	-	-	0.871	0.363	0.377
T5 (11B)	0.164	0.258	0.65	0.045	0.153	0.071	0.064	-	-	0.122	-	-	0.304	0.392	0.331
Cohere small v20220720 (410M)	0.147	0.226	0.361	0.078	0.025	0.074	0.098	0.405	0.238	0.204	-	0.252	0.473	0.434	0.403
babbage (1.3B)	0.117	0.166	0.477	0.255	0.068	0.212	0.149	0.489	0.314	0.162	0.073	0.246	0.5	0.4	0.409
text-ada-001	0.105	0.178	0.332	0.058	0.008	0.034	0.067	0.32	0.248	0.175	0.069	0.252	0.716	0.491	0.335
ada (350M)	0.102	0.204	0.461	0.104	0.031	0.043	0.092	0.37	0.27	0.167	0.072	0.247	0.701	0.421	0.345

