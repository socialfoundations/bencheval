Model	Mean win rate	MMLU - EM (Fairness)	BoolQ - EM (Fairness)	NarrativeQA - F1 (Fairness)	NaturalQuestions (closed-book) - F1 (Fairness)	NaturalQuestions (open-book) - F1 (Fairness)	QuAC - F1 (Fairness)	HellaSwag - EM (Fairness)	OpenbookQA - EM (Fairness)	TruthfulQA - EM (Fairness)	MS MARCO (regular) - RR@10 (Fairness)	MS MARCO (TREC) - NDCG@10 (Fairness)	IMDB - EM (Fairness)	CivilComments - EM (Fairness)	RAFT - EM (Fairness)
Llama 2 (70B)	0.959	0.557	0.859	0.709	0.4	0.637	0.414	-	-	0.434	-	-	0.954	0.551	0.7
LLaMA (65B)	0.924	0.551	0.847	0.661	0.375	0.633	0.333	-	-	0.42	-	-	0.953	0.574	0.668
text-davinci-003	0.903	0.537	0.858	0.664	0.356	0.721	0.45	0.729	0.578	0.491	0.335	0.633	0.833	0.559	0.705
Cohere Command beta (52.4B)	0.866	0.407	0.822	0.657	0.296	0.706	0.316	0.699	0.508	0.222	0.45	0.748	0.957	0.544	0.627
text-davinci-002	0.864	0.531	0.837	0.646	0.32	0.659	0.353	0.703	0.54	0.515	0.373	0.639	0.934	0.463	0.671
Mistral v0.1 (7B)	0.861	0.542	0.842	0.644	0.3	0.625	0.353	-	-	0.332	-	-	0.952	0.52	0.664
Jurassic-2 Jumbo (178B)	0.836	0.45	0.792	0.658	0.327	0.62	0.34	0.655	0.488	0.354	0.342	0.62	0.933	0.507	0.711
LLaMA (30B)	0.822	0.496	0.813	0.657	0.356	0.621	0.325	-	-	0.266	-	-	0.913	0.508	0.718
Llama 2 (13B)	0.808	0.466	0.732	0.657	0.309	0.58	0.351	-	-	0.274	-	-	0.957	0.489	0.673
Palmyra X (43B)	0.797	0.588	0.875	0.651	0.362	-	0.399	-	-	0.542	-	-	0.918	0.006	0.672
Anthropic-LM v4-s3 (52B)	0.794	0.447	0.782	0.646	0.239	0.642	0.356	0.695	0.482	0.3	-	-	0.925	0.512	0.67
TNLG v2 (530B)	0.752	0.418	0.767	0.632	0.318	0.598	0.313	0.678	0.504	0.197	0.341	0.612	0.936	0.48	0.644
MPT (30B)	0.746	0.41	0.631	0.653	0.287	0.624	0.318	-	-	0.19	-	-	0.955	0.553	0.68
gpt-3.5-turbo-0613	0.718	0.313	0.817	0.547	0.287	0.627	0.398	-	-	0.255	-	-	0.912	0.525	0.641
Vicuna v1.3 (13B)	0.715	0.424	0.748	0.607	0.266	0.63	0.324	-	-	0.315	-	-	0.707	0.569	0.62
Falcon-Instruct (40B)	0.709	0.466	0.799	0.543	0.331	0.607	0.308	-	-	0.312	-	-	0.957	0.462	0.561
Jurassic-2 Grande (17B)	0.704	0.433	0.78	0.645	0.283	0.584	0.34	0.632	0.466	0.29	0.243	0.471	0.931	0.445	0.689
MPT-Instruct (30B)	0.687	0.4	0.807	0.633	0.233	0.639	0.252	-	-	0.18	-	-	0.944	0.527	0.636
Falcon (40B)	0.686	0.48	0.783	0.559	0.338	0.625	0.256	-	-	0.292	-	-	0.954	0.292	0.611
J1-Grande v2 beta (17B)	0.677	0.409	0.764	0.647	0.27	0.571	0.308	0.623	0.478	0.242	0.253	0.435	0.95	0.404	0.637
Cohere Command beta (6.1B)	0.662	0.366	0.748	0.595	0.167	0.654	0.273	0.608	0.468	0.163	0.411	0.69	0.95	0.496	0.609
gpt-3.5-turbo-0301	0.662	0.53	0.666	0.585	0.331	0.559	0.417	-	-	0.514	-	-	0.844	0.422	0.689
OPT (175B)	0.622	0.287	0.731	0.573	0.246	0.561	0.266	0.66	0.5	0.203	0.26	0.419	0.944	0.491	0.58
Vicuna v1.3 (7B)	0.622	0.385	0.67	0.553	0.224	0.575	0.304	-	-	0.235	-	-	0.906	0.564	0.643
Llama 2 (7B)	0.61	0.392	0.706	0.596	0.264	0.55	0.321	-	-	0.223	-	-	0.871	0.503	0.609
Cohere xlarge v20221108 (52.4B)	0.608	0.317	0.708	0.553	0.299	0.566	0.275	0.687	0.5	0.12	0.267	0.522	0.949	0.415	0.604
LLaMA (13B)	0.602	0.385	0.666	0.628	0.288	0.561	0.267	-	-	0.234	-	-	0.903	0.533	0.605
davinci (175B)	0.558	0.38	0.682	0.597	0.276	0.567	0.279	0.641	0.502	0.155	0.185	0.357	0.921	0.478	0.605
LLaMA (7B)	0.553	0.284	0.71	0.552	0.241	0.537	0.257	-	-	0.219	-	-	0.936	0.505	0.545
BLOOM (176B)	0.551	0.274	0.656	0.577	0.187	0.575	0.273	0.585	0.482	0.186	0.211	0.371	0.938	0.546	0.563
Cohere xlarge v20220609 (52.4B)	0.55	0.315	0.667	0.548	0.255	0.535	0.281	0.66	0.47	0.156	0.233	0.431	0.949	0.479	0.598
InstructPalmyra (30B)	0.538	0.371	0.7	0.405	0.276	0.63	0.337	-	-	0.152	-	-	0.931	0.449	0.618
Luminous Supreme (70B)	0.522	0.264	0.694	0.603	0.241	0.597	0.288	-	-	0.132	-	-	0.949	0.432	0.601
GLM (130B)	0.513	0.315	0.69	0.615	0.12	0.597	0.205	-	-	0.192	-	-	0.933	0.5	0.575
J1-Jumbo v1 (178B)	0.488	0.236	0.709	0.581	0.235	0.54	0.268	0.614	0.466	0.156	0.18	0.348	0.932	0.478	0.623
Jurassic-2 Large (7.5B)	0.483	0.297	0.685	-	0.217	0.539	-	0.567	0.45	0.196	0.215	0.44	0.945	0.403	0.567
OPT (66B)	0.476	0.229	0.71	0.526	0.218	0.536	0.268	0.597	0.454	0.173	0.214	0.471	0.908	0.5	0.536
RedPajama-INCITE-Instruct (7B)	0.466	0.305	0.616	0.506	0.164	0.592	0.181	-	-	0.183	-	-	0.907	0.54	0.67
J1-Grande v1 (17B)	0.454	0.232	0.678	0.547	0.187	0.521	0.274	0.58	0.472	0.163	0.138	0.328	0.946	0.482	0.636
Luminous Extended (30B)	0.451	0.237	0.711	0.532	0.214	0.551	0.277	-	-	0.16	-	-	0.937	0.462	0.489
Falcon (7B)	0.447	0.261	0.702	0.52	0.233	0.537	0.262	-	-	0.213	-	-	0.794	0.494	0.555
text-curie-001	0.377	0.231	0.576	0.463	0.132	0.5	0.255	0.534	0.452	0.239	0.244	0.482	0.91	0.471	0.458
Alpaca (7B)	0.372	0.346	0.729	0.299	0.21	0.53	0.204	-	-	0.202	-	-	0.699	0.483	0.459
RedPajama-INCITE-Instruct-v1 (3B)	0.369	0.222	0.648	0.506	0.143	0.571	0.183	-	-	0.179	-	-	0.876	0.499	0.632
Cohere large v20220720 (13.1B)	0.362	0.281	0.676	0.512	0.178	0.507	0.256	0.575	0.446	0.157	0.164	0.312	0.92	0.443	0.564
Cohere medium v20221108 (6.1B)	0.34	0.22	0.642	0.497	0.149	0.45	0.229	0.567	0.44	0.182	0.145	0.353	0.917	0.493	0.571
GPT-NeoX (20B)	0.331	0.215	0.609	0.461	0.154	0.525	0.232	0.552	0.438	0.179	0.148	0.381	0.928	0.491	0.475
RedPajama-INCITE-Base (7B)	0.323	0.276	0.65	0.524	0.193	0.514	0.238	-	-	0.17	-	-	0.694	0.431	0.595
Falcon-Instruct (7B)	0.297	0.261	0.637	0.354	0.148	0.383	0.219	-	-	0.183	-	-	0.811	0.502	0.5
TNLG v2 (6.7B)	0.291	0.212	0.665	0.517	0.162	0.501	0.267	0.53	0.412	0.144	0.14	0.317	0.912	0.473	0.502
GPT-J (6B)	0.29	0.22	0.639	0.433	0.122	0.493	0.249	0.486	0.416	0.18	0.129	0.332	0.927	0.488	0.594
J1-Large v1 (7.5B)	0.275	0.204	0.622	0.513	0.146	0.47	0.241	0.528	0.444	0.174	0.117	0.28	0.946	0.447	0.511
RedPajama-INCITE-Base-v1 (3B)	0.27	0.232	0.624	0.42	0.145	0.452	0.238	-	-	0.248	-	-	0.89	0.393	0.475
Cohere medium v20220720 (6.1B)	0.269	0.237	0.597	0.438	0.126	0.432	0.198	0.525	0.42	0.174	0.132	0.357	0.918	0.489	0.5
text-babbage-001	0.244	0.205	0.41	0.299	0.053	0.24	0.196	0.405	0.386	0.207	0.174	0.424	0.887	0.499	0.475
Luminous Base (13B)	0.238	0.185	0.653	0.498	0.16	0.511	0.266	-	-	0.125	-	-	0.912	0.397	0.445
curie (6.7B)	0.231	0.218	0.594	0.482	0.147	0.479	0.243	0.522	0.43	0.186	0.14	0.284	0.86	0.412	0.473
Pythia (12B)	0.226	0.212	0.547	0.449	0.131	0.523	0.227	-	-	0.154	-	-	0.916	0.448	0.489
T0pp (11B)	0.203	0.382	0	0.086	0.028	0.136	0.067	-	-	0.35	-	-	0.168	0.165	0.106
UL2 (20B)	0.186	0.273	0.698	0.053	0.162	0.303	0.107	-	-	0.162	-	-	0.271	0.423	0.375
Pythia (6.9B)	0.171	0.207	0.552	0.389	0.103	0.464	0.198	-	-	0.18	-	-	0.911	0.333	0.45
YaLM (100B)	0.167	0.243	0.583	0.146	0.052	0.177	0.1	-	-	0.202	-	-	0.8	0.456	0.342
Cohere small v20220720 (410M)	0.154	0.222	0.374	0.179	0.055	0.219	0.144	0.308	0.28	0.203	-	0.28	0.518	0.495	0.452
T5 (11B)	0.15	0.235	0.723	0.05	0.159	0.424	0.074	-	-	0.101	-	-	0.303	0.329	0.351
babbage (1.3B)	0.134	0.206	0.436	0.367	0.084	0.381	0.202	0.401	0.326	0.178	0.105	0.301	0.534	0.474	0.438
text-ada-001	0.108	0.202	0.378	0.119	0.012	0.083	0.091	0.27	0.266	0.191	0.107	0.276	0.769	0.497	0.376
ada (350M)	0.105	0.21	0.507	0.205	0.057	0.273	0.166	0.294	0.318	0.185	0.086	0.268	0.806	0.436	0.395

