Model	Mean win rate	MMLU - Denoised inference time (s)	BoolQ - Denoised inference time (s)	NarrativeQA - Denoised inference time (s)	NaturalQuestions (closed-book) - Denoised inference time (s)	NaturalQuestions (open-book) - Denoised inference time (s)	QuAC - Denoised inference time (s)	HellaSwag - Denoised inference time (s)	OpenbookQA - Denoised inference time (s)	TruthfulQA - Denoised inference time (s)	MS MARCO (regular) - Denoised inference time (s)	MS MARCO (TREC) - Denoised inference time (s)	CNN/DailyMail - Denoised inference time (s)	XSUM - Denoised inference time (s)	IMDB - Denoised inference time (s)	CivilComments - Denoised inference time (s)	RAFT - Denoised inference time (s)
text-ada-001	0.938	0.088	0.096	0.171	0.085	0.128	0.21	0.079	0.076	0.089	0.09	0.09	0.793	0.311	0.109	0.092	0.107
curie (6.7B)	0.895	0.092	0.1	0.152	0.122	0.189	0.323	0.084	0.079	0.094	0.094	0.095	0.623	0.294	0.11	0.097	0.112
babbage (1.3B)	0.861	0.119	0.121	0.176	0.152	0.232	0.261	0.113	0.111	0.12	0.122	0.122	0.533	0.272	0.128	0.12	0.137
text-curie-001	0.783	0.133	0.143	0.205	0.153	0.185	0.298	0.125	0.119	0.134	0.136	0.135	0.799	0.364	0.147	0.142	0.152
text-babbage-001	0.778	0.133	0.142	0.243	0.136	0.204	0.314	0.125	0.122	0.134	0.136	0.135	0.968	0.431	0.157	0.138	0.153
ada (350M)	0.77	0.14	0.141	0.211	0.167	0.271	0.27	0.138	0.136	0.141	0.142	0.142	0.598	0.237	0.142	0.141	0.154
text-davinci-002	0.604	0.196	0.191	0.512	0.264	0.394	0.891	0.171	0.158	0.2	0.192	0.198	2.236	1.026	0.247	0.186	0.276
GPT-J (6B)	0.601	0.07	0.499	1.311	1.777	3.866	1.389	0.03	0.019	0.044	0.084	0.081	2.076	0.742	0.701	0.307	0.628
davinci (175B)	0.558	0.212	0.21	0.369	0.327	0.462	1.085	0.193	0.184	0.215	0.211	0.214	2.256	1.148	0.225	0.21	0.279
Cohere medium v20220720 (6.1B)	0.541	0.281	0.35	0.533	0.259	0.535	0.735	0.204	0.187	0.287	0.289	0.288	1.2	0.724	0.452	0.321	0.358
Cohere small v20220720 (410M)	0.534	0.284	0.367	0.56	0.251	0.605	0.619	0.223	0.214	0.289	-	0.291	0.954	0.642	0.458	0.329	0.36
GPT-NeoX (20B)	0.514	0.133	0.773	1.468	0.482	2.137	2.025	0.025	0.024	0.084	0.118	0.116	2.133	1.116	0.862	0.408	1.156
UL2 (20B)	0.506	0.182	0.313	1.182	1.994	3.093	1.226	-	-	0.168	-	-	1.108	0.774	0.215	0.264	0.434
OPT (66B)	0.467	0.055	0.834	1.98	0.611	3.632	2.658	0.971	0.188	0.041	0.076	0.102	1.972	0.885	0.54	0.212	1.871
T5 (11B)	0.434	0.218	0.271	1.054	2.856	12.846	1.032	-	-	0.21	-	-	1.654	1.159	0.278	0.27	0.448
T0pp (11B)	0.42	0.145	0.374	0.945	1.457	2.895	1.239	-	-	0.142	-	-	1.066	0.554	0.393	0.391	0.586
Cohere large v20220720 (13.1B)	0.407	0.317	0.421	0.729	0.337	0.774	1.262	0.225	0.201	0.325	0.33	0.327	2.269	1.075	0.536	0.375	0.444
J1-Large v1 (7.5B)	0.389	0.377	0.485	0.797	0.372	0.733	1.16	0.253	0.238	0.365	0.393	0.389	2.011	0.903	0.637	0.434	0.499
J1-Grande v1 (17B)	0.317	0.411	0.535	0.923	0.466	0.873	1.413	0.33	0.281	0.396	0.428	0.424	2.074	1.07	0.732	0.482	0.59
BLOOM (176B)	0.268	0.233	0.853	2.598	1.115	2.547	5.306	0.075	0.032	0.143	0.257	0.246	5.584	3.9	3.536	0.533	1.866
YaLM (100B)	0.266	0.143	0.828	2.314	2.722	4.463	2.278	-	-	0.092	-	-	2.346	1.671	1.137	0.41	0.89
OPT (175B)	0.241	0.12	0.869	2.783	4.548	7.78	4.049	0.71	0.038	0.141	0.241	0.226	4.729	2.523	1.575	0.498	0.962
J1-Jumbo v1 (178B)	0.222	0.457	0.62	1.126	0.493	1.06	2.064	0.284	0.259	0.443	0.501	0.496	3.777	1.629	0.852	0.552	0.687
Cohere xlarge v20220609 (52.4B)	0.199	0.489	0.598	1.062	0.565	1.085	2.089	0.359	0.314	0.501	0.499	0.501	4.337	1.741	0.796	0.546	0.667
GLM (130B)	0.151	0.335	1.191	2.315	0.953	2.369	4.219	-	-	0.158	-	-	3.514	2.537	1.497	0.695	1.471
Anthropic-LM v4-s3 (52B)	0.138	0.578	0.637	1.722	0.777	1.102	3.694	0.549	0.447	0.568	0.578	0.587	4.076	2.408	0.79	0.594	0.883
J1-Grande v2 beta (17B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Jurassic-2 Jumbo (178B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Jurassic-2 Grande (17B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Jurassic-2 Large (7.5B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Luminous Base (13B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Luminous Extended (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Luminous Supreme (70B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Cohere xlarge v20221108 (52.4B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Cohere medium v20221108 (6.1B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Cohere Command beta (6.1B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Cohere Command beta (52.4B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Pythia (6.9B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Pythia (12B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (13B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
LLaMA (65B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (13B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Llama 2 (70B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Alpaca (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Vicuna v1.3 (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Vicuna v1.3 (13B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Mistral v0.1 (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
TNLG v2 (530B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
TNLG v2 (6.7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
text-davinci-003	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
gpt-3.5-turbo-0301	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
gpt-3.5-turbo-0613	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Base-v1 (3B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Instruct-v1 (3B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Base (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
RedPajama-INCITE-Instruct (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
MPT (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
MPT-Instruct (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon-Instruct (7B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon (40B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Falcon-Instruct (40B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
InstructPalmyra (30B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
Palmyra X (43B)	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-

